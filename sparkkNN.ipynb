{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT__hP5N6e2l"
      },
      "source": [
        "kNN implementation with Spark\n",
        "\n",
        "Goals:\n",
        "* Improve understanding of the kNN algorithm by implementing it\n",
        "* Develop pyspark skills using **core** transformations, and actions\n",
        "* Develop numPy and vectorization skills by using numPy arrays for implementation.\n",
        "* Learn how to write scalable pySpark code by (possibly modifying and) running pySpark code on AWS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zJT9EmuOfEb"
      },
      "source": [
        "## Data\n",
        "\n",
        "The dataset is the widely-used “20 newsgroups” dataset. A newsgroup post is like an old-school blog post, and this dataset has 19,997 such posts from 20 different categories, according to where the blog post was made.\n",
        "\n",
        "The 20 categories are listed in the file `news_categories.txt`.\n",
        "\n",
        "For each document, the category name can be extracted from the id of the document. For example,\n",
        "* the document with the id `20_newsgroups/comp.graphics/37261` is from the `comp.graphics` category,\n",
        "* the document with the id `20_newsgroups/sci.med/59082` is from the `sci.med` category.\n",
        "\n",
        "The data file has one line per document of text. It can be accessed at:\n",
        "\n",
        "`s3://risamyersbucket/Lab/text/20_news_same_line.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-Jz_IPN6kMB"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "This installs Apache Spark, Java, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO2UDWaJ6fpV"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOIDUB-6tex"
      },
      "source": [
        "## Set Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtAHn9Q36kyD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsCqdXOS6yDu"
      },
      "outputs": [],
      "source": [
        "### Start a pySpark Session\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load useful modules\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dW1as8-CMFxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQ4X9Ma7yEn"
      },
      "source": [
        "## Compute \"bag of words\" for each document\n",
        "\n",
        "For task 1, we want to extract \"bag of words\" features for documents.\n",
        "`\n",
        "[('mostcommonword', 0),\n",
        " ('nextmostcommonword', 1),\n",
        " ...]\n",
        "`\n",
        "\n",
        "**NOTE**: There aren’t 20,000 unique words in the small dataset (`20_news_same_line_random_sample.txt`). Use only the top 100 words when working with this file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Provided code to create the reference dictionary of words.**\n",
        "\n",
        "Run the code cells below to create the `refDict` RDD."
      ],
      "metadata": {
        "id": "fXzX6Yl8L4Lp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkRIPgI57t33"
      },
      "outputs": [],
      "source": [
        "# set the number of dictionary words\n",
        "# 100 for the small dataset\n",
        "# 20,000 for the large dataset\n",
        "numWords = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load up the dataset\n",
        "# \"20_news_same_line_random_sample.txt\" for small dataset\n",
        "corpus = sc.textFile (\"20_news_same_line_random_sampleV3.txt\")\n",
        "\n",
        "# each entry in validLines will be a line from the text file\n",
        "validLines = corpus.filter(lambda x : 'id=' in x)\n",
        "\n",
        "# now we transform it into a bunch of (docID, text) pairs\n",
        "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\"> ') + 3:x.index(' </doc>')]))\n",
        "\n",
        "# now we split the text in each (docID, text) pair into a list of words\n",
        "# after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
        "# we have a bit of fancy regular expression stuff here to make sure that we do not\n",
        "# die on some of the documents\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
        "\n",
        "# now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
        "# to (\"word1\", 1) (\"word2\", 1)...\n",
        "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
        "\n",
        "# now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
        "allCounts = allWords.reduceByKey (lambda a, b: a + b)\n",
        "\n",
        "# and get the top numWords (100 for small dataset, 20K for large dataset) frequent words in a local array\n",
        "topWords = allCounts.top (numWords, lambda x : x[1])\n",
        "\n",
        "# and we'll create an RDD that has a bunch of (word, rank) pairs\n",
        "# start by creating an RDD that has the number 0 up to numWords (100 for small dataset, 20K for large dataset)\n",
        "# numWords is the number of words that will be in our dictionary\n",
        "numWordsRDD = sc.parallelize(range(numWords))\n",
        "\n",
        "# now, we transform (0), (1), (2), ... to (\"mostcommonword\", 0) (\"nextmostcommon\", 1), ...\n",
        "# the number will be the spot in the dictionary used to tell us where the word is located\n",
        "refDict = numWordsRDD.map(lambda x:(topWords[x][0],x))"
      ],
      "metadata": {
        "id": "C48Pv5m6L7hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refDict.take(10)"
      ],
      "metadata": {
        "id": "ClQC3N8SMNFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb5b3cf-2852-4b15-9986-6f7e4f52e6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 0),\n",
              " ('to', 1),\n",
              " ('a', 2),\n",
              " ('of', 3),\n",
              " ('and', 4),\n",
              " ('i', 5),\n",
              " ('in', 6),\n",
              " ('is', 7),\n",
              " ('that', 8),\n",
              " ('it', 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtlq1fs972LW"
      },
      "source": [
        "Create a new RDD, named `bag_of_words`. Each element of this RDD corresponds to one document, and is a key-value pair. Specifically, the key is the document identifier `id` (e.g. `20_newsgroups/comp.graphics/37261`) and the value is a `numpy` array with `numWords` (100 for the small dataset, 20K for the large dataset) entries, where the *i*th entry in the array is the number of times that the *i*th word in the `refDict` (created in the first part) appears in the document. This array corresponds to the \"bag of words\" features for each document.\n",
        "\n",
        "Since each array is going to be huge, with a lot of zeros, print out only the non-zero entries in the array (that is, for an array `a`, print out `a[a.nonzero()]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF-ETMv770Ab"
      },
      "outputs": [],
      "source": [
        "# create first RDD to organize from previous RDD in format (word, docID)\n",
        "RDD1= keyAndListOfWords.flatMap(lambda x: ((i, x[0])for i in x[1]))\n",
        "\n",
        "# modify so that refDict is joined with new RDD created, in format (word, (docID, num (rank of word) in refdict)\n",
        "RDD2= RDD1.join(refDict)\n",
        "\n",
        "# # now, remove the characters from new RDD\n",
        "RDD3= RDD2.map(lambda x: ((x[1]))) # (docID, num in refdict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make an array of 0s for length 100\n",
        "zeros= np.zeros(100)\n",
        "\n",
        "# Spark4Actions pg 9 - code all given in slide\n",
        "# first, combine values in chunk against eachother if same\n",
        "def seq_func(accumulator, element):\n",
        "  #sum the values of the same element, counter\n",
        "  accumulator[element]=accumulator[element]+1\n",
        "  return accumulator\n",
        "\n",
        "# next, combine values across chunk against eachother if same\n",
        "def comb_func(accumulator1, accumulator2):\n",
        "  # sum across the partitions\n",
        "  sum= accumulator1+accumulator2\n",
        "  return sum"
      ],
      "metadata": {
        "id": "Cst8sw-WMR2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = RDD3.aggregateByKey (zeros,seq_func, comb_func) #pass 3 things"
      ],
      "metadata": {
        "id": "-2r4-fa7MR8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC0ZIGzF7-rX"
      },
      "source": [
        "Print results by running the code cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZbA4Rv075cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d34a0a-7d2f-4652-b08a-96ed73474c27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.,  2.,  4., 10.,  4.,  5.,  1.,  6.,  7.,  8.,  1.,  1.,  1.,\n",
              "        3.,  2.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
              "        1.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  1.,  1.,\n",
              "        2.,  1.,  2.,  1.,  1.,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "arr1_1 = np.array(bag_of_words.filter(lambda x: \"20_newsgroups/soc.religion.christian/21626\" in x[0]).collect()[0][1])\n",
        "arr1_1[arr1_1.nonzero()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK-aEbi_PyOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de22e5eb-d07a-4337-e08d-2d3eb4b1e790"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7., 23., 17.,  5.,  6.,  5., 14., 10.,  3., 20., 15., 11.,  4.,\n",
              "        1.,  1.,  4.,  4.,  8.,  3.,  4.,  2.,  1.,  2.,  3., 10.,  3.,\n",
              "        1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,\n",
              "        3.,  2.,  1.,  1.,  5.,  2.,  1.,  1.,  1.,  8.,  3.,  1.,  1.,\n",
              "        1.,  2.,  1.,  2.,  1.,  1.,  2.,  1.,  2.,  2.])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "arr1_2 = np.array(bag_of_words.filter(lambda x: \"20_newsgroups/talk.politics.misc/179019\" in x[0]).collect()[0][1])\n",
        "arr1_2[arr1_2.nonzero()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr1_3 = np.array(bag_of_words.filter(lambda x: \"20_newsgroups/rec.autos/103167\" in x[0]).collect()[0][1])\n",
        "arr1_3[arr1_3.nonzero()]"
      ],
      "metadata": {
        "id": "sRVx3l0rMZnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4478a681-eb11-4a86-c619-07cf91450683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9., 1., 2., 3., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
              "       1., 3., 1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gie_jThP8DEm"
      },
      "source": [
        "\n",
        "## Compute the TF-IDF value for each document\n",
        "\n",
        "It is often difficult to classify documents accurately using raw count vectors (bag of words). Thus, the next task is to write some more Spark RDD transformations and actions that convert each of the count vectors to TF-IDF vectors (numpy arrays).\n",
        "\n",
        "Create an RDD of key-value pairs, named `tfidf`, where the keys are document identifiers, and the values are the TF-IDF vector (represented as a numpy array) for that document. Again, we are only interested in the **top** `numWords` (100 for small dataset, 20K for large dataset) most common words.  \n",
        "\n",
        "The *i*th entry in a TF-IDF vector corresponds to the *i*th word in the **top** `numWords` most common words dictionary `refDict`. Then, the *i*th entry in a TF-IDF vector for document $d$ in a corpus with $D$ documents is computed as:\n",
        "\n",
        "$$ TF(i, d) \\times IDF(i, D) $$\n",
        "\n",
        "Where $TF(i, d)$ is:\n",
        "\n",
        "$$ \\frac {\\textrm{# of occurences of word $i$ in $d$}} {\\textrm{Total # of words in $d$}} $$\n",
        "\n",
        "Note that the “Total number of words” is not the number of distinct words. The “total number of words”\n",
        "in “Today is a great day today” is six.\n",
        "\n",
        "And the $IDF(i,D)$ is:\n",
        "\n",
        "$$ \\log \\frac {\\textrm{# of documents in corpus D}} {\\textrm{# of documents having word $i$}} $$\n",
        "\n",
        "Use the numPy [log](https://https://numpy.org/doc/stable/reference/generated/numpy.log.html) function (natural log)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d-sv5M58AmW"
      },
      "outputs": [],
      "source": [
        "# disable exponential notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "numdoc = int(bag_of_words.count()) # first find # of documents in corpus D\n",
        "\n",
        "# create (rank, (id1, id2, ...)) pair\n",
        "rankAndid = RDD2.map(lambda x: (x[1][1], x[1][0]))\n",
        "rankAndidcnt = rankAndid.groupByKey()\n",
        "# create (rank, # of documents having word) pair\n",
        "rankAndocc = rankAndidcnt.map(lambda x: (x[0], len(set(x[1]))))\n",
        "# sort based on rank\n",
        "rankAndoccSorted = rankAndocc.sortBy(lambda x: x[0])\n",
        "# create idf as list using idf formula above\n",
        "idf = rankAndoccSorted.map(lambda x: (np.log(numdoc/x[1]))).collect() # in this case, # of documents in corpus D is numdoc\n",
        "# create tf_idf RDD\n",
        "# for each pair in bag_of_words, calculate TF for each word, then use matrix multiplication with IDF\n",
        "tf_idf = bag_of_words.map(lambda x: (x[0], np.multiply((x[1]/x[1].sum()),idf)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prtUIJky8SqB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFpbNTQe8P-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e336f56-1fb2-429c-c82f-99c02de892af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00498593, 0.0027183 , 0.00558006, 0.02140703, 0.0079482 ,\n",
              "       0.00949249, 0.00155193, 0.0148116 , 0.02311692, 0.02676792,\n",
              "       0.00481739, 0.00495321, 0.01969   , 0.01028608, 0.00595945,\n",
              "       0.00514304, 0.00582799, 0.00662376, 0.01320715, 0.00005222,\n",
              "       0.00733535, 0.0084834 , 0.00157629, 0.00929397, 0.01098612,\n",
              "       0.01003302, 0.02679371, 0.00992003, 0.00911096, 0.01032123,\n",
              "       0.01264814, 0.01089281, 0.01232144, 0.04271363, 0.0116649 ,\n",
              "       0.01250162, 0.02627947, 0.02389596, 0.04994971, 0.01482438,\n",
              "       0.02510224, 0.01420196])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "arr2_1 = tf_idf.filter(lambda x: x[0]=='20_newsgroups/soc.religion.christian/21626').values()\n",
        "arr2_1 = arr2_1.collect()[0]\n",
        "arr2_1[arr2_1.nonzero()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr2_2 = tf_idf.filter(lambda x: x[0]=='20_newsgroups/talk.politics.misc/179019').values()\n",
        "arr2_2 = arr2_2.collect()[0]\n",
        "arr2_2[arr2_2.nonzero()]"
      ],
      "metadata": {
        "id": "W2GKwXALNPzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08df5f62-e489-413c-f7f0-e1ffb63170cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00201045, 0.01260504, 0.00956261, 0.00431593, 0.00480738,\n",
              "       0.00312889, 0.01393565, 0.0133162 , 0.00404757, 0.04404556,\n",
              "       0.01962468, 0.0256024 , 0.00869349, 0.0019425 , 0.00798906,\n",
              "       0.01058602, 0.01820369, 0.00622142, 0.00961201, 0.00414761,\n",
              "       0.00319188, 0.00469999, 0.00578676, 0.02670869, 0.01099002,\n",
              "       0.00002106, 0.00024235, 0.00134919, 0.00684145, 0.0006356 ,\n",
              "       0.00374757, 0.00394377, 0.00357064, 0.00885978, 0.00755923,\n",
              "       0.00486722, 0.01114733, 0.00710066, 0.00510006, 0.00360131,\n",
              "       0.019552  , 0.00627329, 0.00815623, 0.00435498, 0.00496832,\n",
              "       0.03720002, 0.01512293, 0.00467671, 0.00482682, 0.00491113,\n",
              "       0.01107961, 0.00550675, 0.01221835, 0.0055899 , 0.00597757,\n",
              "       0.01145319, 0.00588614, 0.01289577, 0.01345513])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr2_3 = tf_idf.filter(lambda x: x[0]=='20_newsgroups/rec.autos/103167').values()\n",
        "arr2_3 = arr2_3.collect()[0]\n",
        "arr2_3[arr2_3.nonzero()]"
      ],
      "metadata": {
        "id": "o8JJrjHFNP2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45605fb0-48ea-4ee3-ef2c-6391dfe96ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01363931, 0.00289181, 0.00845553, 0.01211807, 0.00660395,\n",
              "       0.00525234, 0.00702642, 0.00690343, 0.01024976, 0.01396454,\n",
              "       0.01094264, 0.01267968, 0.02188527, 0.01405016, 0.00011111,\n",
              "       0.05636151, 0.00127881, 0.00711913, 0.00335381, 0.01873364,\n",
              "       0.02110644, 0.05481859, 0.03012542, 0.11523618, 0.03526483,\n",
              "       0.06288773, 0.03402288])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcvZBMxy8Wr0"
      },
      "source": [
        "## build a kNN classifier\n",
        "\n",
        "This function will take as input a text string (`test_doc`) and a number *k*, and then output the name of one of the 20 newsgroups. This name is the news group category that the classifier thinks that the text string is “closest” to. It is computed using the classical kNN algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ysmi-5w8Ud9"
      },
      "outputs": [],
      "source": [
        "# k is the number of neighbors to consider\n",
        "# test_doc is the text to compare\n",
        "def predictLabel (k, test_doc):\n",
        "    # your code here\n",
        "    regex = re.compile('[^a-zA-Z]')\n",
        "    words = sc.parallelize(regex.sub(' ', test_doc).lower().split()) # first split the doc into list of  # https://docs.python.org/3/library/stdtypes.html\n",
        "    allWordsCnt = words.map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b) # count all of the words to pair (word, cnt)\n",
        "    allWordsCntandRank = allWordsCnt.join(refDict) # join in format of (word, (cnt, rank))\n",
        "\n",
        "    rankandCnt = allWordsCntandRank.map(lambda x: (x[1][1], x[1][0])) # get (rank, cnt) pair\n",
        "\n",
        "    # create a count vector for this test doc, so it is easier to calculate TF using matrix multiplication\n",
        "    countvec = np.zeros(100)\n",
        "    for pair in rankandCnt.collect():\n",
        "      countvec[pair[0]] = pair[1]\n",
        "\n",
        "    tf_idf_test = np.multiply(countvec/countvec.sum(), idf) # calculate TF-IDF for this doc\n",
        "\n",
        "    # calculate l-2 distance\n",
        "    l2_dist = tf_idf.map(lambda x: (x[0], np.linalg.norm(x[1]-tf_idf_test))) # for each doc id, the l2 distance with test doc\n",
        "\n",
        "    knn_candidate = l2_dist.top(k, key = lambda x: -x[1]) # find top k neighbors with min distance\n",
        "\n",
        "    # need to find the most frequent category in those top k\n",
        "    groupnames = [(n[0].split('/')[1], n[1]) for n in knn_candidate] # we only need the category name and dist in this list\n",
        "    category_counts = {}\n",
        "    # summarize the occurance of each category in a dictionary\n",
        "    for cat, dist in groupnames:\n",
        "      if cat in category_counts:\n",
        "        category_counts[cat] += 1 #https://docs.python.org/3/reference/simple_stmts.html\n",
        "      else:\n",
        "        category_counts[cat] = 1\n",
        "\n",
        "    # find the most frequent category\n",
        "    max_count = max(category_counts.values())\n",
        "    most_common_categories = [category for category, count in category_counts.items() if count == max_count]\n",
        "\n",
        "    if len(most_common_categories) == 1: # if no tie happens, then just return the most frequent one\n",
        "      return most_common_categories[0]\n",
        "    else: # if tie happens\n",
        "      min_dist = float('inf')\n",
        "      min_category = None\n",
        "      for category, dist in groupnames: # find the category among the ties with shortest distance\n",
        "          if category in most_common_categories and dist < min_dist:\n",
        "            min_dist = dist\n",
        "            min_category = category\n",
        "      return min_category\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYeOWC28ayk"
      },
      "source": [
        "#### Test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cRSuIJw8Yop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26a7326-59ac-435d-a87b-ca04da1e4e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sci.space\n"
          ]
        }
      ],
      "source": [
        "print(predictLabel (10, 'Graphics are pictures and movies created using computers – usually referring to image data created by a computer specifically with help from specialized graphical hardware and software. It is a vast and recent area in computer science. The phrase was coined by computer graphics researchers Verne Hudson and William Fetter of Boeing in 1960. It is often abbreviated as CG, though sometimes erroneously referred to as CGI. Important topics in computer graphics include user interface design, sprite graphics, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization with ray tracing, and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, and physics. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the user, and processing image data received from the physical world. The interaction and understanding of computers and interpretation of data has been made easier because of computer graphics. Computer graphic development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design generally.'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'A deity is a concept conceived in diverse ways in various cultures, typically as a natural or supernatural being considered divine or sacred. Monotheistic religions accept only one Deity (predominantly referred to as God), polytheistic religions accept and worship multiple deities, henotheistic religions accept one supreme deity without denying other deities considering them as equivalent aspects of the same divine principle, while several non-theistic religions deny any supreme eternal creator deity but accept a pantheon of deities which live, die and are reborn just like any other being. A male deity is a god, while a female deity is a goddess. The Oxford reference defines deity as a god or goddess (in a polytheistic religion), or anything revered as divine. C. Scott Littleton defines a deity as a being with powers greater than those of ordinary humans, but who interacts with humans, positively or negatively, in ways that carry humans to new levels of consciousness beyond the grounded preoccupations of ordinary life.'))\n"
      ],
      "metadata": {
        "id": "Raf0GsZxNcIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735af836-1c27-4db3-b78a-944b6de54034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soc.religion.christian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'Egypt, officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia by a land bridge formed by the Sinai Peninsula. Egypt is a Mediterranean country bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south, and Libya to the west. Across the Gulf of Aqaba lies Jordan, and across from the Sinai Peninsula lies Saudi Arabia, although Jordan and Saudi Arabia do not share a land border with Egypt. It is the worlds only contiguous Eurafrasian nation. Egypt has among the longest histories of any modern country, emerging as one of the worlds first nation states in the tenth millennium BC. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypts rich cultural heritage is an integral part of its national identity, which has endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. One of the earliest centers of Christianity, Egypt was Islamised in the seventh century and remains a predominantly Muslim country, albeit with a significant Christian minority.'))\n"
      ],
      "metadata": {
        "id": "1S4wIwhlNcO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66eec341-c235-4113-d102-328705e925b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sci.space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'The term atheism originated from the Greek atheos, meaning without god(s), used as a pejorative term applied to those thought to reject the gods worshiped by the larger society. With the spread of freethought, skeptical inquiry, and subsequent increase in criticism of religion, application of the term narrowed in scope. The first individuals to identify themselves using the word atheist lived in the 18th century during the Age of Enlightenment. The French Revolution, noted for its unprecedented atheism, witnessed the first major political movement in history to advocate for the supremacy of human reason. Arguments for atheism range from the philosophical to social and historical approaches. Rationales for not believing in deities include arguments that there is a lack of empirical evidence; the problem of evil; the argument from inconsistent revelations; the rejection of concepts that cannot be falsified; and the argument from nonbelief. Although some atheists have adopted secular philosophies (eg. humanism and skepticism), there is no one ideology or set of behaviors to which all atheists adhere.'))\n"
      ],
      "metadata": {
        "id": "2G_scDWhNcU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc3416d-1637-4e14-8bdb-ac2cff6f0abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soc.religion.christian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'President Dwight D. Eisenhower established NASA in 1958 with a distinctly civilian (rather than military) orientation encouraging peaceful applications in space science. The National Aeronautics and Space Act was passed on July 29, 1958, disestablishing NASAs predecessor, the National Advisory Committee for Aeronautics (NACA). The new agency became operational on October 1, 1958. Since that time, most US space exploration efforts have been led by NASA, including the Apollo moon-landing missions, the Skylab space station, and later the Space Shuttle. Currently, NASA is supporting the International Space Station and is overseeing the development of the Orion Multi-Purpose Crew Vehicle, the Space Launch System and Commercial Crew vehicles. The agency is also responsible for the Launch Services Program (LSP) which provides oversight of launch operations and countdown management for unmanned NASA launches.'))\n"
      ],
      "metadata": {
        "id": "-yPO5Z79NlVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59957ba-3a95-4f9e-f730-6e1e4232433e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sci.space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'The transistor is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. First conceived by Julius Lilienfeld in 1926 and practically implemented in 1947 by American physicists John Bardeen, Walter Brattain, and William Shockley, the transistor revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The transistor is on the list of IEEE milestones in electronics, and Bardeen, Brattain, and Shockley shared the 1956 Nobel Prize in Physics for their achievement.'))\n"
      ],
      "metadata": {
        "id": "6ccosGdeNnw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5907354-a5d3-4bc7-dcf9-b006018f83db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.politics.mideast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'The Colt Single Action Army which is also known as the Single Action Army, SAA, Model P, Peacemaker, M1873, and Colt .45 is a single-action revolver with a revolving cylinder holding six metallic cartridges. It was designed for the U.S. government service revolver trials of 1872 by Colts Patent Firearms Manufacturing Company – todays Colts Manufacturing Company – and was adopted as the standard military service revolver until 1892. The Colt SAA has been offered in over 30 different calibers and various barrel lengths. Its overall appearance has remained consistent since 1873. Colt has discontinued its production twice, but brought it back due to popular demand. The revolver was popular with ranchers, lawmen, and outlaws alike, but as of the early 21st century, models are mostly bought by collectors and re-enactors. Its design has influenced the production of numerous other models from other companies.'))\n"
      ],
      "metadata": {
        "id": "1bVHOfb_Nn3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292165e6-9f24-4ee7-cceb-9f80b35156fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sci.crypt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictLabel (10, 'Howe was recruited by the Red Wings and made his NHL debut in 1946. He led the league in scoring each year from 1950 to 1954, then again in 1957 and 1963. He ranked among the top ten in league scoring for 21 consecutive years and set a league record for points in a season (95) in 1953. He won the Stanley Cup with the Red Wings four times, won six Hart Trophies as the leagues most valuable player, and won six Art Ross Trophies as the leading scorer. Howe retired in 1971 and was inducted into the Hockey Hall of Fame the next year. However, he came back two years later to join his sons Mark and Marty on the Houston Aeros of the WHA. Although in his mid-40s, he scored over 100 points twice in six years. He made a brief return to the NHL in 1979–80, playing one season with the Hartford Whalers, then retired at the age of 52. His involvement with the WHA was central to their brief pre-NHL merger success and forced the NHL to expand their recruitment to European talent and to expand to new markets.'))\n"
      ],
      "metadata": {
        "id": "aPGrcY7WNrYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f3656c-2246-4c02-b5d8-c029d435a59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rec.sport.baseball\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyxoOLJx8eyk"
      },
      "source": [
        "## Stop Spark context\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMvQlvmu9VTX"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7fEhb7_Qijk"
      },
      "source": [
        "## run on the entire dataset in EMR cluster"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "numWords = 20000\n",
        "corpus = sc.textFile (\"s3://risamyersbucket/Lab/text/20_news_same_line.txt\")\n",
        "validLines = corpus.filter(lambda x : 'id=' in x)\n",
        "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\"> ') + 3:x.index(' </doc>')]))\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
        "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
        "allCounts = allWords.reduceByKey (lambda a, b: a + b)\n",
        "topWords = allCounts.top (numWords, lambda x : x[1])\n",
        "numWordsRDD = sc.parallelize(range(numWords))\n",
        "refDict = numWordsRDD.map(lambda x:(topWords[x][0],x))\n",
        "RDD1= keyAndListOfWords.flatMap(lambda x: ((i, x[0])for i in x[1]))\n",
        "RDD2= RDD1.join(refDict)\n",
        "RDD3= RDD2.map(lambda x: ((x[1])))\n",
        "zeros= np.zeros(20000)\n",
        "def seq_func(accumulator, element):\n",
        "  accumulator[element]=accumulator[element]+1\n",
        "  return accumulator\n",
        "\n",
        "def comb_func(accumulator1, accumulator2):\n",
        "  sum= accumulator1+accumulator2\n",
        "  return sum\n",
        "\n",
        "bag_of_words = RDD3.aggregateByKey (zeros,seq_func, comb_func)\n",
        "arr1_1 = np.array(bag_of_words.lookup(\"20_newsgroups/soc.religion.christian/21626\"))\n",
        "print(\"20_newsgroups/soc.religion.christian/21626 ... \\n\", arr1_1[arr1_1.nonzero()])\n",
        "arr1_2 = np.array(bag_of_words.lookup(\"20_newsgroups/talk.politics.misc/179019\"))\n",
        "print(\"20_newsgroups/talk.politics.misc/179019 ...\\n\", arr1_2[arr1_2.nonzero()])\n",
        "arr1_3 = np.array(bag_of_words.lookup(\"20_newsgroups/rec.autos/103167\"))\n",
        "print(\"20_newsgroups/rec.autos/103167 ... \\n\", arr1_3[arr1_3.nonzero()])\n",
        "\n",
        "# Task 2\n",
        "np.set_printoptions(suppress=True)\n",
        "rankAndid = RDD2.map(lambda x: (x[1][1], x[1][0]))\n",
        "rankAndidcnt = rankAndid.groupByKey()\n",
        "rankAndocc = rankAndidcnt.map(lambda x: (x[0], len(set(x[1]))))\n",
        "rankAndoccSorted = rankAndocc.sortBy(lambda x: x[0])\n",
        "idf = rankAndoccSorted.map(lambda x: (np.log(19997.0/x[1]))).collect()\n",
        "tfidf = bag_of_words.map(lambda x: (x[0], np.multiply((x[1]/x[1].sum()),idf)))\n",
        "arr2_1 = np.array(tfidf.lookup('20_newsgroups/soc.religion.christian/21626'))\n",
        "print('20_newsgroups/soc.religion.christian/21626 ... \\n', arr2_1[arr2_1.nonzero()])\n",
        "arr2_2 = np.array(tfidf.lookup(\"20_newsgroups/talk.politics.misc/179019\"))\n",
        "print(\"20_newsgroups/talk.politics.misc/179019 ...\\n\", arr2_2[arr2_2.nonzero()])\n",
        "arr2_3 = np.array(tfidf.lookup(\"20_newsgroups/rec.autos/103167\"))\n",
        "print(\"20_newsgroups/rec.autos/103167 ... \\n\", arr2_3[arr2_3.nonzero()])\n",
        "\n",
        "# Task 3\n",
        "def predictLabel (k, test_doc):\n",
        "    regex = re.compile('[^a-zA-Z]')\n",
        "    words = sc.parallelize(regex.sub(' ', test_doc).lower().split())\n",
        "    allWordsCnt = words.map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b)\n",
        "    allWordsCntandRank = allWordsCnt.join(refDict)\n",
        "\n",
        "    rankandCnt = allWordsCntandRank.map(lambda x: (x[1][1], x[1][0]))\n",
        "\n",
        "    countvec = np.zeros(20000)\n",
        "    for pair in rankandCnt.collect():\n",
        "      countvec[pair[0]] = pair[1]\n",
        "\n",
        "    tf_idf_test = np.multiply(countvec/countvec.sum(), idf)\n",
        "\n",
        "    l2_dist = tfidf.map(lambda x: (x[0], np.linalg.norm(x[1]-tf_idf_test)))\n",
        "\n",
        "    knn_candidate = l2_dist.top(k, key = lambda x: -x[1])\n",
        "    groupnames = [(n[0].split('/')[1], n[1]) for n in knn_candidate]\n",
        "    category_counts = {}\n",
        "    for cat, dist in groupnames:\n",
        "      if cat in category_counts:\n",
        "        category_counts[cat] += 1\n",
        "      else:\n",
        "        category_counts[cat] = 1\n",
        "    max_count = max(category_counts.values())\n",
        "    most_common_categories = [category for category, count in category_counts.items() if count == max_count]\n",
        "\n",
        "    if len(most_common_categories) == 1:\n",
        "      return most_common_categories[0]\n",
        "    else:\n",
        "      min_dist = float('inf')\n",
        "      min_category = None\n",
        "      for category, dist in groupnames:\n",
        "          if category in most_common_categories and dist < min_dist:\n",
        "            min_dist = dist\n",
        "            min_category = category\n",
        "      return min_category\n",
        "\n",
        "predictions = []\n",
        "predictions.append(predictLabel (10, 'Graphics are pictures and movies created using computers – usually referring to image data created by a computer specifically with help from specialized graphical hardware and software. It is a vast and recent area in computer science. The phrase was coined by computer graphics researchers Verne Hudson and William Fetter of Boeing in 1960. It is often abbreviated as CG, though sometimes erroneously referred to as CGI. Important topics in computer graphics include user interface design, sprite graphics, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization with ray tracing, and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, and physics. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the user, and processing image data received from the physical world. The interaction and understanding of computers and interpretation of data has been made easier because of computer graphics. Computer graphic development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design generally.'))\n",
        "predictions.append(predictLabel (10, 'A deity is a concept conceived in diverse ways in various cultures, typically as a natural or supernatural being considered divine or sacred. Monotheistic religions accept only one Deity (predominantly referred to as God), polytheistic religions accept and worship multiple deities, henotheistic religions accept one supreme deity without denying other deities considering them as equivalent aspects of the same divine principle, while several non-theistic religions deny any supreme eternal creator deity but accept a pantheon of deities which live, die and are reborn just like any other being. A male deity is a god, while a female deity is a goddess. The Oxford reference defines deity as a god or goddess (in a polytheistic religion), or anything revered as divine. C. Scott Littleton defines a deity as a being with powers greater than those of ordinary humans, but who interacts with humans, positively or negatively, in ways that carry humans to new levels of consciousness beyond the grounded preoccupations of ordinary life.'))\n",
        "predictions.append(predictLabel (10, 'Egypt, officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia by a land bridge formed by the Sinai Peninsula. Egypt is a Mediterranean country bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south, and Libya to the west. Across the Gulf of Aqaba lies Jordan, and across from the Sinai Peninsula lies Saudi Arabia, although Jordan and Saudi Arabia do not share a land border with Egypt. It is the worlds only contiguous Eurafrasian nation. Egypt has among the longest histories of any modern country, emerging as one of the worlds first nation states in the tenth millennium BC. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypts rich cultural heritage is an integral part of its national identity, which has endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. One of the earliest centers of Christianity, Egypt was Islamised in the seventh century and remains a predominantly Muslim country, albeit with a significant Christian minority.'))\n",
        "predictions.append(predictLabel (10, 'The term atheism originated from the Greek atheos, meaning without god(s), used as a pejorative term applied to those thought to reject the gods worshiped by the larger society. With the spread of freethought, skeptical inquiry, and subsequent increase in criticism of religion, application of the term narrowed in scope. The first individuals to identify themselves using the word atheist lived in the 18th century during the Age of Enlightenment. The French Revolution, noted for its unprecedented atheism, witnessed the first major political movement in history to advocate for the supremacy of human reason. Arguments for atheism range from the philosophical to social and historical approaches. Rationales for not believing in deities include arguments that there is a lack of empirical evidence; the problem of evil; the argument from inconsistent revelations; the rejection of concepts that cannot be falsified; and the argument from nonbelief. Although some atheists have adopted secular philosophies (eg. humanism and skepticism), there is no one ideology or set of behaviors to which all atheists adhere.'))\n",
        "predictions.append(predictLabel (10, 'President Dwight D. Eisenhower established NASA in 1958 with a distinctly civilian (rather than military) orientation encouraging peaceful applications in space science. The National Aeronautics and Space Act was passed on July 29, 1958, disestablishing NASAs predecessor, the National Advisory Committee for Aeronautics (NACA). The new agency became operational on October 1, 1958. Since that time, most US space exploration efforts have been led by NASA, including the Apollo moon-landing missions, the Skylab space station, and later the Space Shuttle. Currently, NASA is supporting the International Space Station and is overseeing the development of the Orion Multi-Purpose Crew Vehicle, the Space Launch System and Commercial Crew vehicles. The agency is also responsible for the Launch Services Program (LSP) which provides oversight of launch operations and countdown management for unmanned NASA launches.'))\n",
        "predictions.append(predictLabel (10, 'The transistor is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. First conceived by Julius Lilienfeld in 1926 and practically implemented in 1947 by American physicists John Bardeen, Walter Brattain, and William Shockley, the transistor revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The transistor is on the list of IEEE milestones in electronics, and Bardeen, Brattain, and Shockley shared the 1956 Nobel Prize in Physics for their achievement.'))\n",
        "predictions.append(predictLabel (10, 'The Colt Single Action Army which is also known as the Single Action Army, SAA, Model P, Peacemaker, M1873, and Colt .45 is a single-action revolver with a revolving cylinder holding six metallic cartridges. It was designed for the U.S. government service revolver trials of 1872 by Colts Patent Firearms Manufacturing Company – todays Colts Manufacturing Company – and was adopted as the standard military service revolver until 1892. The Colt SAA has been offered in over 30 different calibers and various barrel lengths. Its overall appearance has remained consistent since 1873. Colt has discontinued its production twice, but brought it back due to popular demand. The revolver was popular with ranchers, lawmen, and outlaws alike, but as of the early 21st century, models are mostly bought by collectors and re-enactors. Its design has influenced the production of numerous other models from other companies.'))\n",
        "predictions.append(predictLabel (10, 'Howe was recruited by the Red Wings and made his NHL debut in 1946. He led the league in scoring each year from 1950 to 1954, then again in 1957 and 1963. He ranked among the top ten in league scoring for 21 consecutive years and set a league record for points in a season (95) in 1953. He won the Stanley Cup with the Red Wings four times, won six Hart Trophies as the leagues most valuable player, and won six Art Ross Trophies as the leading scorer. Howe retired in 1971 and was inducted into the Hockey Hall of Fame the next year. However, he came back two years later to join his sons Mark and Marty on the Houston Aeros of the WHA. Although in his mid-40s, he scored over 100 points twice in six years. He made a brief return to the NHL in 1979–80, playing one season with the Hartford Whalers, then retired at the age of 52. His involvement with the WHA was central to their brief pre-NHL merger success and forced the NHL to expand their recruitment to European talent and to expand to new markets.'))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "BkOmkBTIN5t3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}