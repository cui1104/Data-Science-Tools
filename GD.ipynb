{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In the following code, we generate a set of data points along a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a function that generates linear data\n",
    "def genLinearData(x, slope, intercept):\n",
    "    return slope * x + intercept\n",
    "\n",
    "# and here's a function that adds Gaussian noise to a set of values\n",
    "def addGaussianNoise(data, mu, sigma):\n",
    "    ''' \n",
    "        data is the data to add noise to\n",
    "        mu is the median of the Gaussian\n",
    "        sigma is the standard deviation of the Gaussian\n",
    "    '''\n",
    "    return data + np.random.normal(mu, sigma, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a data set for linear regression\n",
    "numPoints = 500 # number of points\n",
    "\n",
    "# evenly spaced x values\n",
    "x = np.linspace(-2,2,numPoints)\n",
    "\n",
    "# \"true\" y values, based on the slope, intercept, and x values\n",
    "y = genLinearData(x, slope=5, intercept=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add some Gaussian noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "# mu = 0\n",
    "# sigma = 5\n",
    "yNoisy = addGaussianNoise(y, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,yNoisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task One: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "def gd(X, Y, startM, startB, numIters, epsilon, lRate):\n",
    "    \"\"\"\n",
    "    gradient descent algorithm for linear regression using mean squared error. \n",
    "    Uses all of the data in each iteration\n",
    "    \n",
    "     Returns\n",
    "        The number of iterations run\n",
    "        The final value of lRate\n",
    "        The final prediction for the slope\n",
    "        The final prediction for the intercept\n",
    "\n",
    "    Parameters:\n",
    "        X - input data\n",
    "        Y - true output data\n",
    "        startM - starting value for the line slope\n",
    "        startB - starting value for the line intercept\n",
    "        numIters - maximum number of iterations to run\n",
    "        epsilon - Stopping condition: change in loss function from iteration to iteration\n",
    "        lRate - learning rate\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run implementation of Gradient Descent\n",
    "\n",
    "Generate data using the following parameters:\n",
    "\n",
    "* x range: -2 to 2\n",
    "* 500 points\n",
    "* slope = 5\n",
    "* intercept = -1\n",
    "\n",
    "Gaussian noise:\n",
    "* mu = 0\n",
    "* sigma = 5\n",
    "\n",
    "Run implementation of gradient descent on data generated with these aforementioned parameters.\n",
    "\n",
    "Start with values \n",
    "* slope = -12\n",
    "* intercept = 0\n",
    "* max iterations = 1000\n",
    "* epsilon = .01\n",
    "* learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed before generating the data and running your algorithm\n",
    "np.random.seed(553)\n",
    "\n",
    "numPoints = 500\n",
    "# evenly spaced x values\n",
    "x = np.linspace(-2,2,numPoints)\n",
    "y = genLinearData(x, slope=5, intercept=-1)\n",
    "yNoisy = yNoisy = addGaussianNoise(y, mu=0, sigma=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "iters, myM, myB = gd(x, yNoisy, startM=-12, startB=0, numIters=1000, epsilon=.01, lRate=0.1)\n",
    "end = time.time()\n",
    "\n",
    "print('time: ', round(end - start,4))\n",
    "print('m: ', round(myM,2), \n",
    "      'b: ', round(myB,2),\n",
    "      'iters: ', iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Two Gradient Descent with Bold Driver\n",
    "\n",
    "Return:\n",
    "\n",
    "* the number of iterations actually run\n",
    "\n",
    "* the final learning rate\n",
    "\n",
    "* the predicted value for the slope\n",
    "\n",
    "* the predicted value for the intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdBoldDriver(X, Y, startM, startB, numIters, epsilon, lRate):\n",
    "    \"\"\"\n",
    "    gradient descent algorithm for linear regression using mean squared error. \n",
    "    Uses all of the data in each iteration.\n",
    "    Implements the Bold Driver algorithm\n",
    "    Returns\n",
    "        The number of iterations run\n",
    "        The final learning rate\n",
    "        The final prediction for the slope\n",
    "        The final prediction for the intercept\n",
    "    \n",
    "    Parameters:\n",
    "        X - input data\n",
    "        Y - true output data\n",
    "        startM - starting value for the line slope\n",
    "        startB - starting value for the line intercept\n",
    "        numIters - maximum number of iterations to run\n",
    "        epsilon - Stopping condition: change in loss function from iteration to iteration\n",
    "        lRate - learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run implementation of Gradient Descent with Bold Driver\n",
    "using the same parameters as last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "iters, lRate, myM, myB = gdBoldDriver(x, yNoisy, startM=-12, startB=0, numIters=1000, epsilon=.01, lRate=0.1)\n",
    "end = time.time()\n",
    "\n",
    "print('time: ', round(end - start,4))\n",
    "print('m: ', round(myM,2), \n",
    "      'b: ', round(myB,2),\n",
    "      'iters: ', iters, \n",
    "      'lambda: ', round(lRate,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Three: Mini-batch Gradient Descent\n",
    "\n",
    "Implement mini-batch Gradient Descent for linear regression, using a parameterized batch size at each iteration. Also, add a flag that indicates if Bold Driver should be used.\n",
    "\n",
    "Return:\n",
    "\n",
    "* the number of iterations actually run\n",
    "\n",
    "* the final learning rate\n",
    "\n",
    "* the predicted value for the slope\n",
    "\n",
    "* the predicted value for the intercept\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdMiniBatch(X, Y, startM, startB, numIters, epsilon, lRate, batchSize, withBD):\n",
    "    \"\"\"\n",
    "    Mini batch gradient descent algorithm for linear regression using mean squared error. \n",
    "    Uses a random selection of batchSize data points in each iteration.\n",
    "    Returns\n",
    "        The number of iterations run\n",
    "        The final learning rate\n",
    "        The final prediction for the slope\n",
    "        The final prediction for the intercept\n",
    "    \n",
    "    Parameters:\n",
    "        X - input data\n",
    "        Y - true output data\n",
    "        startM - starting value for the line slope\n",
    "        startB - starting value for the line intercept\n",
    "        numIters - maximum number of iterations to run\n",
    "        epsilon - Stopping condition: change in loss function from iteration to iteration\n",
    "        lRate - starting learning rate\n",
    "        batchSize - the number of points to use at each iteration\n",
    "        withBD - flag set to 1 if Bold Driver should be used\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    # dataset size\n",
    "    n = len(X)\n",
    "\n",
    "    # randomly select a batch\n",
    "    # this code gives you a set of indexes you can use on X and Y \n",
    "    thisBatch = np.random.choice(n, batchSize, replace=False)\n",
    "\n",
    "    # your code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run implementation of Mini-batch Gradient Descent\n",
    "\n",
    "using the same parameters as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "# keep track of values across iterations\n",
    "totalTime = 0\n",
    "totalIters = 0\n",
    "totalLRate = 0\n",
    "totalM = 0\n",
    "totalB = 0\n",
    "totalMisses = 0\n",
    "\n",
    "numTimes = 100\n",
    "for iter in range(numTimes):\n",
    "    start = time.time()\n",
    "    iters, lRate, myM, myB = gdMiniBatch(x, yNoisy, startM=-12, startB=0, numIters=1000, \\\n",
    "                                         epsilon=.01, lRate=0.1, batchSize = 10, withBD=0)\n",
    "    end = time.time()\n",
    "\n",
    "    if # your code here. How do you know if there was a miss?\n",
    "        totalMisses += 1\n",
    "    else:\n",
    "        totalTime += end - start\n",
    "        totalIters += iters\n",
    "        totalLRate += lRate\n",
    "        totalM += myM\n",
    "        totalB += myB\n",
    "\n",
    "\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', round(totalIters/(numTimes-totalMisses),0), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4),\n",
    "      'total misses: ', totalMisses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Mini-Batch Gradient Descent **with Bold Driver**, using a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', round(totalIters/(numTimes-totalMisses),0), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4),\n",
    "      'total misses: ', totalMisses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Four Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent is just mini-batch GD, run with a batch size of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "# your code here\n",
    "\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', round(totalIters/(numTimes-totalMisses),0), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4),\n",
    "      'total misses: ', totalMisses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run implementation of Stochastic Gradient Descent with Bold Driver\n",
    "\n",
    "Again, run the algorithm 100 times and report back averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', round(totalIters/(numTimes-totalMisses),0), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),6),\n",
    "      'total misses: ', totalMisses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task Five: Unknown data\n",
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# you may get an error if you do not have 3rd party cookies enabled\n",
    "# you may use the files pane on the left to upload the file instead\n",
    "files.upload()\n",
    "\n",
    "df = pd.read_csv('GDdata.csv',header=None, names=['newX', 'newY'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['newX'],df['newY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the algorithm with:\n",
    "\n",
    "* slope = -1\n",
    "* intercept = -4\n",
    "* learning rate 0.01\n",
    "* max iterations 10000\n",
    "\n",
    "### Run Gradient Descent on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "iters, myM, myB = gd(df['newX'], df['newY'], startM=-1, startB=-4, numIters=10000, epsilon=.01, lRate=.01)\n",
    "end = time.time()\n",
    "\n",
    "print('time: ', round(end - start,4))\n",
    "print('m: ', round(myM,2), \n",
    "      'b: ', round(myB,2),\n",
    "      'iters: ', iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent with Bold Driver on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "iters, lRate, myM, myB = gdBoldDriver(df['newX'], df['newY'], startM=-1, startB=-4, numIters=500, epsilon=.01, lRate=0.01)\n",
    "end = time.time()\n",
    "print('time: ', round(end - start,4))\n",
    "print('m: ', round(myM,2), \n",
    "      'b: ', round(myB,2),\n",
    "      'iters: ', iters, \n",
    "      'lambda: ', round(lRate,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Mini-batch Gradient Descent on the data\n",
    "\n",
    "Run 50 iterations and report average results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "# your code here\n",
    "start = time.time()\n",
    "\n",
    "iters, lRate, myM, myB = gdMiniBatch(df['newX'], df['newY'], startM=-1, startB=-4, numIters=1000, \\\n",
    "                                     epsilon=.01, lRate=0.01, batchSize = 1, withBD=0)\n",
    "end = time.time()\n",
    "# your code here\n",
    "\n",
    "\n",
    "print('misses: ', totalMisses)\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', iters, \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Mini-batch Gradient Descent with Bold Driver on the data\n",
    "\n",
    "Run 50 iterations and report average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "\n",
    "# your code here\n",
    "\n",
    "start = time.time()\n",
    "iters, lRate, myM, myB = gdMiniBatch(df['newX'], df['newY'], startM=-1, startB=-4, numIters=1000, \\\n",
    "                                     epsilon=.01, lRate=0.01, batchSize = 1, withBD=1)\n",
    "end = time.time()\n",
    "# your code here\n",
    "\n",
    "\n",
    "print(totalIters/(numTimes-totalMisses), totalLRate/(numTimes-totalMisses), totalM/(numTimes-totalMisses), totalB/(numTimes-totalMisses))\n",
    "print('misses: ', totalMisses)\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', totalIters/(numTimes-totalMisses), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stochastic Gradient Descent on the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "start = time.time()\n",
    "# your code here\n",
    "iters, lRate, myM, myB = gdMiniBatch(df['newX'], df['newY'], startM=.1, startB=0, numIters=2000, \\\n",
    "                                     epsilon=.01, lRate=0.05, batchSize = 1, withBD=0)\n",
    "\n",
    "end = time.time()\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('misses: ', totalMisses)\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', round(totalIters/(numTimes-totalMisses),2), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stochastic Gradient Descent with Bold Driver on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this approach is stochastic, set the seed again, so we all get the same answers\n",
    "np.random.seed(553)\n",
    "\n",
    "# your code here\n",
    "iters, lRate, myM, myB = gdMiniBatch(df['newX'], df['newY'], startM=.1, startB=0, numIters=2000, \\\n",
    "                                     epsilon=.01, lRate=0.05, batchSize = 1, withBD=1)\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "iters, lRate, myM, myB = gdMiniBatch()\n",
    "end = time.time()\n",
    "\n",
    "print('misses: ', totalMisses)\n",
    "print('time: ', round(totalTime/(numTimes-totalMisses),4))\n",
    "print('m: ', round(totalM/(numTimes-totalMisses),2), \n",
    "      'b: ', round(totalB/(numTimes-totalMisses),2),\n",
    "      'iters: ', round(totalIters/(numTimes-totalMisses),0), \n",
    "      'lambda: ', round(totalLRate/(numTimes-totalMisses),4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
